{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "zs7UKBPSFh1E",
   "metadata": {
    "id": "zs7UKBPSFh1E"
   },
   "source": [
    "# 2. Déploiement de la solution sur le cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcf53ce-2690-4804-806a-e6c39294d274",
   "metadata": {},
   "source": [
    "## 2.1 Installations et Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KA5YCDehfi6b",
   "metadata": {
    "id": "KA5YCDehfi6b"
   },
   "source": [
    "### 2.1.1 Installation des packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f90483-0d92-4b72-9a14-de586c9abd30",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "Ej0UNd7-BM-i",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "7c25cdc6-15fe-4ee1-95b1-9fadeb78e7a5"
   },
   "source": [
    "Les packages nécessaires ont été installé via l'étape de **bootstrap** à l'instanciation du serveur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d60aaa4-fc31-41e5-9e07-8a3f29ebb7f6",
   "metadata": {
    "id": "9d60aaa4-fc31-41e5-9e07-8a3f29ebb7f6"
   },
   "source": [
    "### 2.1.2 Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf8e939-2c44-4c3e-8471-8d9b07cf14bd",
   "metadata": {
    "id": "6bf8e939-2c44-4c3e-8471-8d9b07cf14bd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import io\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30467d6-8c36-485e-b358-8eeb894dae0b",
   "metadata": {
    "id": "f30467d6-8c36-485e-b358-8eeb894dae0b"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50,  preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras import Model\n",
    "from pyspark.sql.functions import col, udf, pandas_udf, PandasUDFType, element_at, split\n",
    "from keras.layers import Input, Dense\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e619845d-748f-4951-babf-f3d6da717f00",
   "metadata": {
    "id": "e619845d-748f-4951-babf-f3d6da717f00"
   },
   "source": [
    "### 2.1.3 Définition des PATH \n",
    "\n",
    "Nous accédons directement à nos **données sur S3** comme si elles étaient **stockées localement**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043e2c74-c1fb-4212-93f5-b42062972417",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 's3://colab-notebook-p8'\n",
    "PATH_Data = PATH+'/Data'\n",
    "PATH_Preprocessing = PATH+'/Preprocessing'\n",
    "PATH_Result = PATH+'/Results'\n",
    "\n",
    "print('PATH:        '+\\\n",
    "      PATH+'\\nPATH_Data:   '+\\\n",
    "      PATH_Data+'\\nPATH_Preprocessing: '+\\\n",
    "      PATH_Preprocessing +\\\n",
    "      '\\nPATH_Result: '+PATH_Result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfe67ce-0512-4ca6-bb1f-c0c8744aa872",
   "metadata": {},
   "source": [
    "### 2.1.4 Démarrage de la session Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef02861-b6b9-45b9-822d-03dffe41ee67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import input_file_name\n",
    "\n",
    "# Initialiser la session Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"P8\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pPvwVA3OYBxA",
   "metadata": {
    "id": "pPvwVA3OYBxA"
   },
   "source": [
    "## 2.3 Chargement des images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e68f1e9-f286-40f8-ab8c-2d91fe26a559",
   "metadata": {},
   "source": [
    "<u> Chargement des images </u>\n",
    "\n",
    "Nous allons charger tous les fichiers d'images JPEG depuis un stockage S3,</br> y compris tous les sous-répertoires, dans un DataFrame Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc17f69-e017-40ff-87be-aa08f99f4c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import input_file_name\n",
    "\n",
    "# Lire les images depuis S3\n",
    "images = spark.read.format(\"binaryFile\") \\\n",
    "    .option(\"pathGlobFilter\", \"*.jpg\") \\\n",
    "    .option(\"recursiveFileLookup\", \"true\") \\\n",
    "    .load(PATH_Data)\n",
    "\n",
    "images.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc45cc98-1c35-4e5b-9aea-9114e4359741",
   "metadata": {},
   "source": [
    "<u>Je ne conserve que le **path** de l'image et j'ajoute <br />\n",
    "    une colonne contenant les **labels** de chaque image</u> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85297d8d-27fd-4d1e-97f7-a84a949f2882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter une colonne 'label' à partir du chemin\n",
    "image_df = images.withColumn('label', element_at(split(images['path'], '/'), -2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b74fd4-529b-43fd-aca7-45d842b8afe6",
   "metadata": {},
   "source": [
    "<u> Affichages des images </u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e893293-1628-4b5a-91ab-d35a295abd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher le schéma du DataFrame\n",
    "image_df.printSchema()\n",
    "\n",
    "# Afficher quelques exemples d'images\n",
    "print(\"Afficher des images\")\n",
    "print(image_df.select('path','label').show(5,False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-SlsqfVOfdUI",
   "metadata": {
    "id": "-SlsqfVOfdUI"
   },
   "source": [
    "## 2.4 Préprocessing des images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e9d0c2-75c6-4f8c-9f65-13b8c1548f3b",
   "metadata": {},
   "source": [
    "### 2.4.1 Préparation du modèle\n",
    "\n",
    "Je vais utiliser la technique du **transfert learning** pour extraire les features des images.<br />\n",
    "J'ai choisi d'utiliser le modèle **ResNet 50** pour sa rapidité d'exécution <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b9b77f-dc19-4bd2-a3dd-e99c85dc87dd",
   "metadata": {},
   "source": [
    "Nous chargeons le modèle ResNet50 avec les poids **précalculés** <br/>\n",
    "issus d'imagenet, incluant la **couche de classification finale** <br/>  et en spécifiant le format de nos images en entrée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c74367-2992-441b-aaf6-10e70735a602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le modèle et diffuser les poids\n",
    "model = ResNet50(weights='imagenet',\n",
    "                    include_top=True,\n",
    "                    input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c6feb4-d685-417d-b134-972842e18c02",
   "metadata": {},
   "source": [
    "Nous créons un nouveau modèle avec:<br/>\n",
    "+ en entrée : l'entrée du modèle ResNet50 <br/>\n",
    "+ en sortie : l'avant dernière couche du modèle ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57466833-548d-4e0c-a563-bd1c788ad4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = Model(inputs=model.input,\n",
    "                      outputs=model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285d3eff-c527-4bb0-91c6-b843f22c02d6",
   "metadata": {},
   "source": [
    "La méthode broadcast de SparkContext permet de créer un objet de diffusion qui <br/> est partagé entre tous les nœuds du cluster <br/>\n",
    "Tous les workeurs doivent pouvoir accéder au modèle ainsi qu'à ses poids. <br />\n",
    "Une bonne pratique consiste à charger le modèle sur le driver puis à diffuser <br />\n",
    "ensuite les poids aux différents workeurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73524657-adf6-45d9-beb5-5cfd5fc100aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "brodcast_weights = spark.sparkContext.broadcast(new_model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8674e803-ec3d-451f-bdce-92f3a7368270",
   "metadata": {},
   "source": [
    "Affichage du résumé de notre nouveau modèle où nous constatons <br />\n",
    "que <u>nous récupérons bien en sortie un vecteur de dimension (1, 1, 1280)</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56045b6-0692-49c2-898b-a67b396d0ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ae922f-6160-4cc9-bb3b-27998153033a",
   "metadata": {},
   "source": [
    "<u>Mettons cela sous forme de fonction</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cb9296-ff55-4ca3-a570-d92ce811efe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn():\n",
    "    \"\"\"\n",
    "    Returns a ResNet50 model with top layer removed \n",
    "    and broadcasted pretrained weights.\n",
    "    \"\"\"\n",
    "    # Créer le modèle ResNet50 avec les poids préentraînés sur ImageNet\n",
    "    model = ResNet50(weights='imagenet',\n",
    "                     include_top=True,\n",
    "                     input_shape=(224, 224, 3))\n",
    "    \n",
    "    # Gel des couches pour éviter leur entraînement\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Créer un nouveau modèle sans la couche de classification finale\n",
    "    new_model = Model(inputs=model.input,\n",
    "                      outputs=model.layers[-2].output)\n",
    "    \n",
    "    # Affecter les poids préentraînés (si nécessaire)\n",
    "    new_model.set_weights(brodcast_weights.value)  \n",
    "\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "T7u2gEN8JuLO",
   "metadata": {
    "id": "T7u2gEN8JuLO"
   },
   "source": [
    "### 2.4.2  Processus de chargement des images <br/> et application de leur featurisation à travers l'utilisation de pandas UDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JbeFrHh8nGZT",
   "metadata": {
    "id": "JbeFrHh8nGZT"
   },
   "source": [
    "#### 2.4.2.1 Processus de chargement des images.\n",
    "\n",
    " L'utilisation des pandas UDFs dans Apache Spark pour intégrer des modèles de machine learning <br/> est un excellent moyende traitement de données. <br/> En permettant le traitement par lots et en évitant le rechargement du modèle à chaque itération,<br/> cette approche optimise les performances et la gestion des ressources. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2ced10-46ee-4acc-93b1-aee8b2320e15",
   "metadata": {},
   "source": [
    " La fonction **preprocess** est conçue pour préparer des images brutes <br />\n",
    " (sous forme de bytes) afin qu'elles puissent être utilisées pour faire des prédictions avec un modèle de machine learning.<br /> Cela inclut le redimensionnement de l'image et la conversion en un format approprié."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c42168a-1d6c-4c63-b321-72913061277d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(content):\n",
    "    try:\n",
    "        # Ouvrir l'image à partir des octets\n",
    "        img = Image.open(io.BytesIO(content)).resize([224, 224])\n",
    "        # convertir l'image en tableau Tableau NumPy de la forme (224, 224, 3)\n",
    "        arr = img_to_array(img)\n",
    "        return preprocess_input(arr)\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du traitement de l'image : {e}\")\n",
    "        return np.zeros((224, 224, 3))  # Retourner un tableau vide ou une image par défaut"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e8d858-2e09-4775-8953-1e5331529f86",
   "metadata": {},
   "source": [
    "La fonction **featurize_series** prend un modèle de machine learning <br /> et une série de données d'images brutes (sous forme de pd.Series de pandas)<br /> et renvoie les caractéristiques extraites de ces images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bed4128-a2d6-4baa-8b53-411cab2f3432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize_series(model, content_series):\n",
    "    # Appliquer la fonction de prétraitement et empiler les résultats\n",
    "    input = np.stack(content_series.map(preprocess))\n",
    "    preds = model.predict(input)\n",
    "     # Aplatir les tenseurs de caractéristiques pour un stockage plus facile\n",
    "    output = [p.flatten() for p in preds]\n",
    "    return pd.Series(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6a698e-0c63-47bb-b032-fcdbe6f326e7",
   "metadata": {},
   "source": [
    " La fonction **featurize_udf** est un User Defined Function (UDF) <br /> pour Apache Spark qui permet d'appliquer la fonction featurize_series à des séries de données d'images dans un contexte distribué.<br /> Cela permet de traiter efficacement de grandes quantités d'images en parallèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb9ab44-7207-4b54-b006-1cd553f91dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(ArrayType(FloatType()), PandasUDFType.SCALAR_ITER)\n",
    "def featurize_udf(content_series_iter):\n",
    "    model = model_fn()\n",
    "    for content_series in content_series_iter:\n",
    "        yield featurize_series(model, content_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b716ba46-9d8b-47c3-961b-9448cd616e1e",
   "metadata": {},
   "source": [
    "#### 2.4.2.2 Exécution des actions d'extraction de features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823df101-c5ae-427e-8c6f-c57ab22cb4b3",
   "metadata": {},
   "source": [
    "Les Pandas UDF, sur de grands enregistrements (par exemple, de très grandes images), <br />\n",
    "peuvent rencontrer des erreurs de type Out Of Memory (OOM).<br />\n",
    "Si vous rencontrez de telles erreurs dans la cellule ci-dessous, <br />\n",
    "essayez de réduire la taille du lot Arrow via 'maxRecordsPerBatch'\n",
    "\n",
    "Je n'utiliserai pas cette commande dans ce projet <br />\n",
    "et je laisse donc la commande en commentaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1a287cb0-a40d-43a1-9159-1e6e8d442bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"1024\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea730d1-43c7-4318-9923-63ac3f44328b",
   "metadata": {},
   "source": [
    "Nous pouvons maintenant exécuter la featurisation sur l'ensemble de notre DataFrame Spark.<br />\n",
    "<u>REMARQUE</u> : Cela peut prendre beaucoup de temps, tout dépend du volume de données à traiter. <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Oxpvu2qFnSeH",
   "metadata": {
    "id": "Oxpvu2qFnSeH"
   },
   "source": [
    "<u> Appliquer l'UDF et récupérer les prédictions </u>\n",
    "\n",
    "Créons un nouveau DataFrame predictions_df qui contient trois colonnes dont une seule colonne, \"features\",</br> remplie des prédictions générées par le modèle pour chaque entrée dans la colonne \"content\" du DataFrame d'origine image_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be68f879-27c9-42f6-be89-32fe3bde3b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = image_df.repartition(24).select(col(\"path\"),\n",
    "                                            col(\"label\"),\n",
    "                                            featurize_udf(\"content\").alias(\"features\")\n",
    "                                           )\n",
    "print(f\"Nombre de lignes dans predictions_df : {predictions_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4ad2de-3b23-414f-9276-aef87a8f8487",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df.write.mode(\"overwrite\").parquet(PATH_Preprocessing)\n",
    "print(\"Enregistrement de predictions_df effectué avec succès\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0751836a-ca34-4b42-8332-a5a73be881bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions_df.show(2,False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mOo7XdSpprQ4",
   "metadata": {
    "id": "mOo7XdSpprQ4"
   },
   "source": [
    "## 2.5 Appliquer le PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3eecd3-dc7f-4f9c-97bd-a20e9fcdd83b",
   "metadata": {},
   "source": [
    " La colonne **\"features\"** dans le DataFrame predictions_df est mise à jour</br> pour contenir des **vecteurs denses** au lieu de listes ou de tableaux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HWAaxp720Gmy",
   "metadata": {
    "id": "HWAaxp720Gmy"
   },
   "outputs": [],
   "source": [
    "@udf(returnType=VectorUDT())\n",
    "def to_vector(arr):\n",
    "  return Vectors.dense(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97889885-c4a2-4425-a03d-32c8a10d1f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = predictions_df.withColumn(\"features\", to_vector(col(\"features\")))\n",
    "print(\"Mise à jour avec lec vecteurs denses\")\n",
    "# Vérification des résultats\n",
    "predictions_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7deb4c-3c32-4b68-9b68-4fd4ac8f83e2",
   "metadata": {},
   "source": [
    "<u> PCA </u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7374c9-5def-4606-ad0b-7586f564e623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer une instance de PCA\n",
    "pca = PCA(k=5, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "\n",
    "# Ajuster le modèle PCA\n",
    "pca_model = pca.fit(predictions_df)\n",
    "\n",
    "# Transformer les données\n",
    "pca_result = pca_model.transform(predictions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2df09f-1b69-4928-a9cf-7b234cb4ebc5",
   "metadata": {},
   "source": [
    "<u> Enregistrement des resultats </u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2323483c-6745-4d6f-a71f-72687ac013f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enregistrer les résultats au format \"Parquet\" \n",
    "pca_result.write.mode(\"overwrite\").parquet(PATH_Result)\n",
    "print(\"Enregistrement au format Parquet effectué avec succès\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a56b49-eab9-474c-ab7a-f14ce7180b15",
   "metadata": {},
   "source": [
    "## 2.6 Chargement des données enregistrées"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b928033-8f89-4d51-8bbe-54e147bea5ad",
   "metadata": {},
   "source": [
    "<u>On charge les données fraichement enregistrées dans un **DataFrame Spark**</u>  :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85631561-8a85-4d9c-b64f-518c1c697fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(PATH_Result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c15fe40-a7ed-426d-aa59-5aeb37dfcd3e",
   "metadata": {},
   "source": [
    "<u>On affiche les 2 premières lignes du DataFrame</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33a7ad3-c8f5-489f-969a-1ac27d061f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Afficher les resultats\")\n",
    "df.show(2,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3cba27-503e-494d-b8e0-af8a24d5aa2e",
   "metadata": {},
   "source": [
    "<u>On affiche les dimension du DataFrame Spark</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23a21db-0833-4bb2-82dc-edc4e5e136b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Afficher les dimensions\")\n",
    "num_rows = df.count()\n",
    "num_columns = len(df.columns)\n",
    "print(f\"Dimensions: ({num_rows}, {num_columns})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba40d6e-104d-4a9b-bf2c-6feaaa3ad7a0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2.7 Arrêt de la session spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "U9lEnDC6yBKV",
   "metadata": {
    "id": "U9lEnDC6yBKV"
   },
   "outputs": [],
   "source": [
    "# Arrêter Spark\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
